from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
from openai import OpenAI
from dotenv import load_dotenv
import os
import subprocess
import json
import tempfile
from pydantic import BaseModel, Field
import traceback

load_dotenv()

# Middleware para manejar el prefijo /testdenivel
class PrefixMiddleware:
    def __init__(self, app, prefix=''):
        self.app = app
        self.prefix = prefix

    def __call__(self, environ, start_response):
        if environ['PATH_INFO'].startswith(self.prefix):
            environ['PATH_INFO'] = environ['PATH_INFO'][len(self.prefix):]
            environ['SCRIPT_NAME'] = self.prefix
        return self.app(environ, start_response)

app = Flask(__name__)
app.wsgi_app = PrefixMiddleware(app.wsgi_app, prefix='/testdenivel')
CORS(app)
client = OpenAI()

CHAR_LIMIT = 3000  # L√≠mite de caracteres por prompt

# üëá Modelo estructurado para respuesta de evaluaci√≥n
class Observacion(BaseModel):
    tipo: str  # "fortaleza" o "consejo"
    texto: str

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/transcribe", methods=["POST"])
def transcribe():
    try:
        audio_file = request.files["file"]
        print("üì• Archivo recibido:", audio_file.filename)

        # Crear archivos temporales √∫nicos
        in_temp = tempfile.NamedTemporaryFile(suffix=".webm", delete=False)
        out_temp = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)

        input_path = in_temp.name
        output_path = out_temp.name

        audio_file.save(input_path)
        print("üìÅ Guardado en:", input_path)

        # Convertir a WAV, limitado a 35s si quer√©s mantener ese tope
        print("üîÑ Ejecutando ffmpeg...")
        subprocess.run(["ffmpeg", "-y", "-t", "35", "-i", input_path, output_path], check=True)
        print("‚úÖ Conversi√≥n completada:", output_path)

        # Enviar a Whisper API de OpenAI
        print("üì§ Enviando a OpenAI para transcripci√≥n...")
        with open(output_path, "rb") as f:
            transcription = client.audio.transcriptions.create(
                model="gpt-4o-mini-transcribe",
                file=f
            )

        print("üìù Transcripci√≥n:", transcription.text)

        return jsonify({
            "text": transcription.text
        })

    except Exception as e:
        print("‚ùå Error en /transcribe:", e)
        return jsonify({"error": str(e)}), 500

    finally:
        # Borrar archivos temporales si existen
        for path in [input_path, output_path]:
            try:
                os.remove(path)
                print(f"üßπ Archivo temporal eliminado: {path}")
            except Exception as err:
                print(f"‚ö†Ô∏è No se pudo borrar {path}:", err)

class DistribucionCEFR(BaseModel):
    A1: int
    A2: int
    B1: int
    B2: int
    C1: int

    def suma_valida(self) -> bool:
        return sum(self.dict().values()) == 100

class EvaluacionFinal(BaseModel):
    nivel: str
    mensaje: str
    observaciones: list[Observacion]

class TurnoEvaluado(BaseModel):
    reply: str = Field(..., description="Pregunta generada para el usuario")
    Q: float = Field(..., description="Calidad informativa (de 0 a 1)")
    P_nueva: DistribucionCEFR

@app.route("/chat", methods=["POST"])
def chat():
    try:
        data = request.json
        prompt = data["message"][:CHAR_LIMIT]
        print("üí¨ Prompt recibido:\n", prompt)

        completion = client.beta.chat.completions.parse(
            model="gpt-5-nano",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "Eres una entrevistadora especializada en evaluaci√≥n oral de idiomas mediante una conversaci√≥n breve y natural.\n\n"
                        "Rol:\n"
                        "- Habla SIEMPRE en el idioma objetivo indicado en el mensaje de usuario.\n"
                        "- Genera solo UNA intervenci√≥n corta con UNA pregunta clara.\n"
                        "- Adapta la dificultad de tu intervenci√≥n al nivel CEFR estimado que se pasa en el mensaje de usuario.\n"
                        "- Usa el tema/situaci√≥n, historial reciente y √∫ltimo mensaje para dar fluidez y NO repetir preguntas. Pero NO te quedes pegado, ampl√≠a la conversaci√≥n\n\n"
                        "Evaluaci√≥n:\n"
                        "- Eval√∫a SOLO la √∫ltima respuesta del usuario.\n"
                        "- Calcula Q ‚àà [0,1], la calidad de informaci√≥n que aporta SOLO la √∫ltima respuesta para estimar el nivel de idioma :\n"
                        "  ‚Ä¢ Q ‚â§ 0.3 ‚Üí respuesta con informaci√≥n casi nula para evaluar nivel (muy corta, gen√©rica, memorizada, evasiva).\n"
                        "  ‚Ä¢ 0.4 < Q < 0.7 ‚Üí respuesta con algo de informaci√≥n, pero limitada o desarrollo).\n"
                        "  ‚Ä¢ Q ‚â• 0.7 ‚Üí respuesta suficientemente rica: clara, con contenido real, suficiente para extraer evidencias de nivel\n"
                        "- Calcula P_nueva = distribuci√≥n CEFR (A1‚ÄìC1) que refleje SOLO esta √∫ltima respuesta.\n"
                        "  ‚Ä¢ Siempre suma 100 con valores enteros.\n"
                        "  ‚Ä¢ Respuesta con muchos errores o sin sentido ‚Üí peso en A1"
                        "  ‚Ä¢ Respuesta muy b√°sica, con vocabulario limitado, algunos errores o frases sueltas ‚Üí peso en A1/A2.\n"
                        "  ‚Ä¢ Respuesta con ideas completas y control b√°sico ‚Üí peso en A2/B1.\n"
                        "  ‚Ä¢ Respuesta con fluidez y conectores ‚Üí peso en B1/B2.\n"
                        "  ‚Ä¢ Respuesta con estructuras avanzadas, matiz y precisi√≥n ‚Üí peso en B2/C1.\n"
                        "  ‚Ä¢ Respuesta con gran dominio del idioma ‚Üí peso en C1.\n"
                        "Si es la primera respuesta del usuario dale Q = 0"
                        "Si no es la primera respuesta del usuarios y Q ‚â§ 0.4, antes de tu pregunta incluye un consejo breve y emp√°tico en espa√±ol (m√°x. 1 l√≠nea).\n\n"
                        "Formato JSON estricto (sin texto extra):\n"
                        "{\n"
                        '  "reply": "tu pr√≥xima intervenci√≥n en el idioma objetivo",\n'
                        '  "Q": n√∫mero entre 0 y 1,\n'
                        '  "P_nueva": { "A1": %, "A2": %, "B1": %, "B2": %, "C1": % }\n'
                        "}"
                    )
                },
                {"role": "user", "content": prompt}
            ],
            response_format=TurnoEvaluado
        )

        parsed = completion.choices[0].message.parsed

        if not parsed.P_nueva.suma_valida():
            raise ValueError("‚ùå La distribuci√≥n CEFR no suma 100.")

        print("‚úÖ JSON estructurado recibido:", parsed)
        return jsonify(parsed.dict())


    except Exception as e:
        print("‚ùå Error en /chat:", e)
        traceback.print_exc()  # üëà esto te da el stack completo
        return jsonify({"error": str(e)}), 500

@app.route("/tts", methods=["POST"])
def tts():
    try:
        data = request.json
        print("üî£ TTS input:", data["text"])

        audio = client.audio.speech.create(
            model="tts-1",
            voice="nova",
            input=data["text"]
        )

        print("üîä Audio TTS generado")
        return audio.content, 200, {"Content-Type": "audio/mpeg"}

    except Exception as e:
        print("‚ùå Error en /tts:", e)
        return jsonify({"error": str(e)}), 500

@app.route("/evaluate", methods=["POST"])
def evaluate():
    try:
        data = request.json
        print("üìä Recibiendo historial para evaluaci√≥n final...")

        messages = [
            {
                "role": "system",
                "content": (
                   "Eres un evaluador profesional de nivel de ingl√©s. Basado en el historial completo de conversaci√≥n entre un candidato y una entrevistadora, entrega una evaluaci√≥n final clara, en espa√±ol, usando este formato JSON:\n\n"
                    "{\n"
                    "  \"nivel\": \"B1 - Intermedio\",\n"
                    "  \"mensaje\": \"Mensaje resumen emp√°tico y directo (m√°x 1 frase)\",\n"
                    "  \"observaciones\": [\n"
                    "    { \"tipo\": \"fortaleza\", \"texto\": \"...\" },\n"
                    "    { \"tipo\": \"fortaleza\", \"texto\": \"...\" },\n"
                    "    { \"tipo\": \"consejo\", \"texto\": \"...\" },\n"
                    "    { \"tipo\": \"consejo\", \"texto\": \"...\" }\n"
                    "  ]\n"
                    "}\n\n"
                    "Eval√∫a con justicia. Si el usuario mostr√≥ frases muy b√°sicas, errores constantes o respuestas muy cortas, puedes asignar A1.\n"
                    "Si respondi√≥ con fluidez y estructuras avanzadas, puedes asignar C1.\n"
                    "Si respondi√≥ poco, o con baja calidad, igualmente debes entregar un diagn√≥stico y consejos.\n\n"
                    "Las observaciones deben ser √∫tiles, breves (m√°x 150 caracteres), y basadas en fragmentos reales de la conversaci√≥n.\n"
                    "Usa un tono amable, profesional y constructivo. No incluyas explicaciones fuera del JSON."
                )
            },
            {
                "role": "user",
                "content": f"Este es el historial completo:\n\n{json.dumps(data['historial'], indent=2)}"
            }
        ]

        completion = client.beta.chat.completions.parse(
            model="gpt-5-mini",
            messages=messages,
            response_format=EvaluacionFinal,
        )

        parsed = completion.choices[0].message.parsed

        return jsonify(parsed.dict())

    except Exception as e:
        print("‚ùå Error en /evaluate:", e)
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    print("üöÄ Servidor iniciado en modo debug")
    app.run(debug=True)
